# -*- coding: utf-8 -*-
"""Customer Chrun.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nj8ywiT2jZFnsOgoWClmcdbLyUcX5Iw8
"""

# Cell 1: Import Libraries
# This cell imports all the necessary libraries for data manipulation, visualization,
# machine learning models, and evaluation metrics.
import pandas as pd  # For data manipulation and analysis (e.g., DataFrames)
import numpy as np  # For numerical operations, especially for mathematical functions like log
import matplotlib.pyplot as plt  # For creating static, interactive, and animated visualizations
import seaborn as sns  # For statistical data visualization, built on Matplotlib
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  # Tools for splitting data, cross-validation, and hyperparameter tuning
from sklearn.preprocessing import LabelEncoder, StandardScaler  # For encoding categorical data and scaling numerical features
from sklearn.metrics import (
    classification_report,  # Comprehensive report of precision, recall, f1-score, and support
    confusion_matrix,  # Table summarizing prediction results
    accuracy_score,  # Proportion of correctly classified instances
    precision_score,  # Proportion of true positive predictions among all positive predictions
    recall_score,  # Proportion of true positive predictions among all actual positives (sensitivity)
    f1_score,  # Harmonic mean of precision and recall
    roc_auc_score,  # Area Under the Receiver Operating Characteristic Curve
    make_scorer # Utility to create a scorer from a custom function
)
from sklearn.ensemble import RandomForestClassifier, VotingClassifier  # Ensemble models
from sklearn.linear_model import LogisticRegression  # Linear model for binary classification
from sklearn.neighbors import KNeighborsClassifier  # Non-parametric lazy learning algorithm
from sklearn.tree import DecisionTreeClassifier  # Tree-based model for classification
from xgboost import XGBClassifier  # Optimized distributed gradient boosting library
from sklearn.neural_network import MLPClassifier  # Multi-layer Perceptron (feedforward artificial neural network)
from sklearn.metrics import roc_curve, auc # For plotting ROC curves and calculating Area Under Curve

# Cell 2: Data Collection (Acquisition)
# This cell is responsible for loading the dataset into a pandas DataFrame.

# Define the path to your dataset file.
import kagglehub
import os

# Download latest version
data_dir = kagglehub.dataset_download("abdullah0a/telecom-customer-churn-insights-for-analysis")

print("Path to dataset files:", data_dir)

# Construct the full path to the CSV file within the downloaded directory
data_path = os.path.join(data_dir, 'customer_churn_data.csv')


# Load the CSV file into a pandas DataFrame.
df = pd.read_csv(data_path)

# Display the first 5 rows of the DataFrame to get a quick overview of the data.
print("First 5 rows of the dataset:")
print(df.head())

# Display the shape of the DataFrame (number of rows, number of columns).
print("\nShape of the dataset (rows, columns):", df.shape)

# Cell 3: Data Preparation - Missing Values and Duplicates
# This cell handles missing values and removes duplicate rows to ensure data quality.

# Check for and display the count of missing values in each column before any cleaning.
print("\nMissing values per column before handling:")
print(df.isnull().sum())

# Remove any duplicate rows from the DataFrame.
# This ensures that each customer observation is unique.
df = df.drop_duplicates()
print(f"\nShape of the dataset after dropping duplicates: {df.shape}")

# Handle missing values in categorical (object) columns by imputing them with the mode.
# The mode is the most frequently occurring value.
for column in df.select_dtypes(include=['object']).columns:
    mode_value = df[column].mode()[0]  # Get the most frequent value
    df[column] = df[column].fillna(mode_value)  # Fill NaN values with the mode

# Verify that all missing values have been successfully handled.
# The output should show all zeros for missing counts.
print("\nMissing values after handling (should be all zeros):")
print(df.isnull().sum())

# Cell 4: Data Preparation - Outlier Detection and Visualization
# This cell defines a function to detect outliers and then visualizes them using boxplots.

def detect_outliers_all_columns(df_input):
    """
    Detects outliers in all numerical columns of a DataFrame using the IQR method.

    Parameters:
    - df_input (pd.DataFrame): The input DataFrame.

    Returns:
    - dict: A dictionary where keys are column names and values are the count of outliers.
    """
    numerical_columns = df_input.select_dtypes(include=['float', 'int']).columns
    outliers_summary = {}

    for column in numerical_columns:
        Q1 = df_input[column].quantile(0.25)  # 25th percentile
        Q3 = df_input[column].quantile(0.75)  # 75th percentile
        IQR = Q3 - Q1  # Interquartile Range

        # Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Count outliers outside the boundaries
        outliers_count = len(df_input[(df_input[column] < lower_bound) | (df_input[column] > upper_bound)])
        outliers_summary[column] = outliers_count
    return outliers_summary

# Detect outliers in all numerical columns of the DataFrame.
outliers_summary = detect_outliers_all_columns(df)
print("\nOutliers detected in each numerical column:")
print(outliers_summary)

# Identify columns that have detected outliers for visualization.
columns_with_outliers = [column for column, count in outliers_summary.items() if count > 0]

# Visualize outliers using boxplots for better understanding of their distribution.
if columns_with_outliers:
    num_columns = len(columns_with_outliers)
    cols_plot = 3
    rows_plot = int(np.ceil(num_columns / cols_plot))

    plt.figure(figsize=(5 * cols_plot, rows_plot * 6)) # Adjust figure size dynamically

    for i, column in enumerate(columns_with_outliers, 1):
        plt.subplot(rows_plot, cols_plot, i) # Create a subplot for each column
        sns.boxplot(data=df, y=column) # Generate a boxplot
        plt.title(f"Boxplot for {column}") # Set subplot title
        plt.xlabel("Data Distribution")
        plt.ylabel("Values")

    plt.tight_layout() # Adjust layout to prevent titles/labels from overlapping
    plt.show()
else:
    print("\nNo numerical columns with outliers detected to plot.")

# Cell 5: Data Preparation - Type Conversion
# This cell ensures all relevant columns are converted to a numerical format suitable for machine learning.

# Print current data types to see what needs conversion.
print("\nData types before comprehensive conversion:")
print(df.dtypes)

def convert_to_numeric(df_input):
    """
    Converts various column types in a DataFrame to a numerical format.
    - 'object' (categorical) columns are Label Encoded.
    - 'datetime64[ns]' columns are converted to days since the earliest date.
    - 'bool' columns are converted to integers (0s and 1s).

    Parameters:
    - df_input (pd.DataFrame): The input DataFrame.

    Returns:
    - pd.DataFrame: The DataFrame with converted columns.
    """
    df_converted = df_input.copy() # Work on a copy to avoid modifying the original DataFrame directly
    for column in df_converted.columns:
        if df_converted[column].dtype == 'object':
            # Use LabelEncoder for categorical columns (e.g., 'Gender', 'Partner', 'InternetService')
            le = LabelEncoder()
            df_converted[column] = le.fit_transform(df_converted[column].astype(str))
        elif df_converted[column].dtype == 'datetime64[ns]':
            # Convert datetime columns to a numerical representation (e.g., days since start)
            df_converted[column] = (df_converted[column] - df_converted[column].min()) / np.timedelta64(1, 'D')
        elif df_converted[column].dtype == 'bool':
            # Convert boolean columns to integers (True -> 1, False -> 0)
            df_converted[column] = df_converted[column].astype(int)
    return df_converted

# Apply the conversion function to the DataFrame.
df = convert_to_numeric(df)

# Verify that all data types are now numerical or in an appropriate format.
print("\nData types after comprehensive conversion:")
print(df.dtypes)

# Display the first few rows again to observe the changes after type conversion.
print("\nFirst 5 rows of the DataFrame after type conversion:")
print(df.head())

# Cell 6: Feature Engineering and Scaling
# This cell performs feature transformation (log transformation) and feature scaling.

# Log Transformation for Skewed Numerical Columns.
# Log transformation can help in normalizing skewed distributions, which can improve
# the performance of models sensitive to feature distribution (e.g., Logistic Regression).
# np.log1p(x) is used for values close to zero as it computes log(1+x).
skewed_columns = ['TotalCharges', 'MonthlyCharges']
for column in skewed_columns:
    if column in df.columns:
        # Apply transformation only if value is positive; otherwise, keep as is (or 0)
        df[column] = df[column].apply(lambda x: np.log1p(x) if x > 0 else 0)
    else:
        print(f"Warning: Column '{column}' not found for log transformation.")

# Feature Scaling using StandardScaler.
# StandardScaler standardizes features by removing the mean and scaling to unit variance.
# This is important for algorithms that rely on distance metrics (e.g., KNN, Neural Networks)
# or gradient descent (e.g., Logistic Regression) to prevent features with larger scales
# from dominating the learning process.
scaler = StandardScaler()
numeric_features_to_scale = ['Tenure', 'MonthlyCharges', 'TotalCharges']

# Add 'Age' to features to scale if it exists and is numerical.
# This check ensures robustness if 'Age' column name varies or is not present.
if 'Age' in df.columns and pd.api.types.is_numeric_dtype(df['Age']):
    numeric_features_to_scale.append('Age')

# Ensure unique features and only select columns that actually exist in the DataFrame.
numeric_features_to_scale = list(set(numeric_features_to_scale) & set(df.columns))

# Apply StandardScaler to the selected numerical features.
df[numeric_features_to_scale] = scaler.fit_transform(df[numeric_features_to_scale])

# Feature Creation: Average Charge Per Month.
# This new feature might provide additional predictive power by combining existing information.
# Adding 1 to 'Tenure' to avoid division by zero if a customer's tenure is 0.
df['AvgChargePerMonth'] = df['TotalCharges'] / (df['Tenure'] + 1)

# Drop Redundant Features.
# 'CustomerID' is typically a unique identifier and does not contain predictive information
# for customer churn. Dropping it prevents it from being treated as a feature.
if 'CustomerID' in df.columns:
    df = df.drop(columns=['CustomerID'])
else:
    print("Warning: 'CustomerID' column not found for dropping.")

# Display the first few rows of the DataFrame after all transformations and engineering.
print("\nFirst 5 rows of the DataFrame after feature engineering and scaling:")
print(df.head())

# Cell 7: Data Exploration - Correlation Heatmap
# This cell visualizes the correlation between all numerical features using a heatmap.

# Compute the pairwise correlation between all columns in the DataFrame.
correlation_matrix = df.corr()

# Set up the figure size for the heatmap for better readability.
plt.figure(figsize=(12, 8))

# Create a heatmap using seaborn.
# annot=True displays the correlation values on the map.
# cmap='coolwarm' uses a color scheme that highlights positive (red) and negative (blue) correlations.
# fmt='.2f' formats the annotation values to two decimal places.
# cbar=True displays the color bar.
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Correlation Heatmap after Preprocessing', fontsize=16) # Set the title of the heatmap
plt.show() # Display the plot

# Cell 8: Data Splitting (Train-Test Split)
# This cell separates features from the target variable and splits the data into training and testing sets.

# Separate features (X) from the target variable (y).
# 'Churn' is the target variable (what we want to predict).
X = df.drop(columns=['Churn'])  # All columns except 'Churn' are features
y = df['Churn']  # 'Churn' column is the target

# Split the dataset into training and testing sets.
# X_train, y_train: Used for training the machine learning models.
# X_test, y_test: Used for evaluating the performance of the trained models on unseen data.
# test_size=0.2 means 20% of the data will be used for testing, and 80% for training.
# random_state=42 ensures reproducibility of the split, so you get the same split every time.
# stratify=y ensures that the proportion of 'Churn' (target) classes is maintained
# in both training and test sets, which is crucial for imbalanced datasets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Print the shapes of the resulting datasets to confirm the split.
print(f"Training set features shape: {X_train.shape}")
print(f"Training set target shape: {y_train.shape}")
print(f"Test set features shape: {X_test.shape}")
print(f"Test set target shape: {y_test.shape}")

# Cell 9: Model Initialization and Initial Evaluation
# This cell initializes various classification models and evaluates their performance on the test set.

# Initialize a dictionary of different machine learning models.
# Each model is set with a random_state for consistent results where applicable.
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),  # n_neighbors is a hyperparameter for KNN
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),  # n_estimators is number of trees
    "XGB": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),  # eval_metric for older versions of XGBoost
    "NeuralNetwork": MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=500, random_state=42) # hidden_layer_sizes defines the network architecture
}

# List to store the evaluation results for each model.
results = []

# Loop through each model, train it, make predictions, and evaluate its performance.
for name, model in models.items():
    print(f"\nTraining and evaluating {name}...")

    # Train the model using the training data (X_train, y_train).
    # Neural Networks are sensitive to feature scaling. While global scaling was done,
    # if using different scalers per model or specific scaling needs, handle here.
    # For this notebook's flow, X_train is already pre-scaled, so we use it directly.
    model.fit(X_train, y_train)

    # Make predictions on the unseen test data (X_test).
    y_pred = model.predict(X_test)

    # Calculate various classification metrics to assess model performance.
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Store the calculated metrics in the results list.
    results.append({
        "Model": name,
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1
    })

# Cell 10: Ensemble Modeling (VotingClassifier)
# This cell implements a VotingClassifier to combine the predictions of multiple models.

# Create a VotingClassifier.
# It combines predictions from Logistic Regression, KNN, Decision Tree, Random Forest, and XGBoost.
# 'soft' voting uses predicted probabilities, which often leads to better performance than 'hard' voting (majority vote).
# n_jobs=-1 utilizes all available CPU cores for parallel processing during training.
voting_clf = VotingClassifier(estimators=[
    ("LogisticRegression", models["LogisticRegression"]),
    ("KNN", models["KNN"]),
    ("DecisionTree", models["DecisionTree"]),
    ("RandomForest", models["RandomForest"]),
    ("XGB", models["XGB"])
], voting='soft', n_jobs=-1)

print("\nTraining and evaluating VotingClassifier...")
# Train the ensemble model on the training data.
voting_clf.fit(X_train, y_train)

# Make predictions on the test set using the trained ensemble model.
y_pred_voting = voting_clf.predict(X_test)

# Evaluate the performance of the Voting Classifier.
accuracy_voting = accuracy_score(y_test, y_pred_voting)
precision_voting = precision_score(y_test, y_pred_voting)
recall_voting = recall_score(y_test, y_pred_voting)
f1_voting = f1_score(y_test, y_pred_voting)

# Add the VotingClassifier's results to the overall results list.
results.append({
    "Model": "VotingClassifier",
    "Accuracy": accuracy_voting,
    "Precision": precision_voting,
    "Recall": recall_voting,
    "F1-Score": f1_voting
})

# Convert the list of results into a pandas DataFrame for structured display.
results_df = pd.DataFrame(results)

# Sort the results DataFrame by F1-Score in descending order.
# F1-Score is often a good metric for imbalanced classification tasks as it balances precision and recall.
results_df = results_df.sort_values(by="F1-Score", ascending=False, ignore_index=True)

# Print the comprehensive evaluation results for all individual models and the ensemble.
print("\n--- Model Evaluation Results (Initial Run) ---")
print(results_df)

# Identify and print the best performing model based on F1-Score from this initial run.
best_model_initial = results_df.iloc[0]
print("\nBest Model (Initial Run based on F1-Score):")
print(best_model_initial)

# Cell 11: Cross-Validation Example
# This cell demonstrates how to use cross-validation for a more robust performance estimate.

print("\n--- Cross-Validation Example (Random Forest) ---")
# Initialize a RandomForestClassifier for cross-validation.
rf_model_for_cv = RandomForestClassifier(random_state=42)

# Perform 5-fold cross-validation.
# cv=5 means the data will be split into 5 parts; the model will be trained on 4 parts
# and tested on 1 part, repeating this 5 times.
# scoring='accuracy' specifies the evaluation metric.
# n_jobs=-1 uses all available CPU cores for faster computation.
rf_scores = cross_val_score(rf_model_for_cv, X, y, cv=5, scoring='accuracy', n_jobs=-1)

# Print the individual cross-validation scores.
print(f"Random Forest Cross-Validation Accuracy Scores: {rf_scores}")
# Print the mean (average) accuracy across all folds, which is a more stable estimate.
print(f"Random Forest Mean Cross-Validation Accuracy: {rf_scores.mean():.4f}")
# Print the standard deviation, indicating the variability of the scores.
print(f"Random Forest Standard Deviation of Cross-Validation Accuracy: {rf_scores.std():.4f}")

# Cell 12: Feature Importance
# This cell calculates and displays the importance of each feature for a tree-based model (Random Forest).

print("\n--- Feature Importance (Random Forest) ---")
# Re-fit a RandomForestClassifier to ensure feature importances are calculated on a trained model.
rf_for_importance = RandomForestClassifier(n_estimators=100, random_state=42)
rf_for_importance.fit(X_train, y_train)

# Create a DataFrame to display feature importances.
# Feature importances indicate how much each feature contributes to the model's predictions.
feature_importances = pd.DataFrame({
    'Feature': X.columns,  # All feature names
    'Importance': rf_for_importance.feature_importances_  # The importance score for each feature
}).sort_values(by='Importance', ascending=False)  # Sort by importance in descending order

print(feature_importances)

# Cell 13: Hyperparameter Tuning with GridSearchCV
# This cell uses GridSearchCV to find the optimal hyperparameters for the RandomForestClassifier.

print("\n--- Hyperparameter Tuning (GridSearchCV) for Random Forest ---")

# Define the parameter grid (combinations of hyperparameters to test).
param_grid_rf = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 10, 20],  # Maximum depth of each tree (None means full depth)
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
}

# Initialize GridSearchCV.
# estimator: The model to tune (RandomForestClassifier).
# param_grid: The dictionary of hyperparameters to search.
# cv: Number of folds for cross-validation during tuning.
# scoring: Metric to optimize (e.g., 'accuracy').
# n_jobs=-1: Use all available CPU cores for parallel computation.
# verbose=1: Display progress messages during the search.
grid_search_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit GridSearchCV on the training data. This will perform the exhaustive search.
grid_search_rf.fit(X_train, y_train)

# Print the best parameters found by GridSearchCV.
print(f"\nBest parameters for Random Forest: {grid_search_rf.best_params_}")
# Print the best cross-validation score achieved with these optimal parameters.
print(f"Best cross-validation accuracy for Random Forest: {grid_search_rf.best_score_:.4f}")

# Cell 14: Comprehensive Model Evaluation Function
# This cell defines a reusable function to evaluate models with various classification metrics,
# including a robust ROC-AUC calculation and plotting.

def evaluate_model(model, X_data, y_true):
    """
    Evaluates a given machine learning model on the provided dataset and returns performance metrics.

    Parameters:
    - model: Trained machine learning model to evaluate.
    - X_data: Features dataset (e.g., X_test or X_train).
    - y_true: True target labels for the dataset (e.g., y_test or y_train).

    Returns:
    - Dictionary containing evaluation metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC.
    """
    # Make predictions on the provided data
    y_pred = model.predict(X_data)

    # Calculate standard classification metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    # Calculate ROC-AUC if the model provides probability estimates or decision function scores.
    roc_auc = 0.0 # Default value if ROC-AUC cannot be computed
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_data)[:, 1]  # Get probabilities for the positive class (class 1)
        roc_auc = roc_auc_score(y_true, y_proba)
    elif hasattr(model, "decision_function"):
        y_proba = model.decision_function(X_data) # Use decision_function for models like SVMs
        roc_auc = roc_auc_score(y_true, y_proba)

    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1,
        "ROC-AUC": roc_auc
    }

def plot_roc_curves(models_dict, X_test_data, y_test_true):
    """
    Plots the ROC curve for each model in the provided dictionary.

    Parameters:
    - models_dict (dict): A dictionary where keys are model names (str) and values are trained model objects.
    - X_test_data (pd.DataFrame): Test features.
    - y_test_true (pd.Series): True labels for the test set.
    """
    plt.figure(figsize=(10, 8))
    for name, model in models_dict.items():
        # Check if the model can provide probability estimates or decision scores
        if hasattr(model, "predict_proba"):
            y_proba = model.predict_proba(X_test_data)[:, 1]  # Probabilities for the positive class
        elif hasattr(model, "decision_function"):
            y_proba = model.decision_function(X_test_data)  # Decision function scores for models like SVM
        else:
            print(f"Skipping ROC curve for {name} as it does not support predict_proba or decision_function.")
            continue

        # Compute ROC curve metrics
        fpr, tpr, _ = roc_curve(y_test_true, y_proba)
        roc_auc = auc(fpr, tpr)

        # Plot the ROC curve for the current model
        plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

    # Plot the diagonal line representing a random guess
    plt.plot([0, 1], [0, 1], color="gray", linestyle="--", label="Random Guess (AUC = 0.50)")

    plt.title("ROC Curves for Models")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

# Add the trained VotingClassifier to the models dictionary for comprehensive evaluation
models["VotingClassifier"] = voting_clf

# Plot ROC curves for all models to visually compare their performance.
plot_roc_curves(models, X_test, y_test)

print("\n--- Final Comprehensive Evaluation of All Models ---")
final_evaluation_results = []
for name, model in models.items():
    metrics = evaluate_model(model, X_test, y_test)
    final_evaluation_results.append({"Model": name, **metrics})

final_results_df = pd.DataFrame(final_evaluation_results)
final_results_df = final_results_df.sort_values(by="F1-Score", ascending=False, ignore_index=True)
print(final_results_df)